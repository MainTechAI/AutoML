{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932de179",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import arff\n",
    "from pymfe.mfe import MFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "    family product-type steel  carbon  hardness temper_rolling condition  \\\n0        ?            C     A       8         0              ?         S   \n1        ?            C     R       0         0              ?         S   \n2        ?            C     R       0         0              ?         S   \n3        ?            C     A       0        60              T         ?   \n4        ?            C     A       0        60              T         ?   \n..     ...          ...   ...     ...       ...            ...       ...   \n893      ?            C     R       0         0              ?         S   \n894      ?            C     R       0         0              ?         S   \n895      ?            C     V       0         0              ?         S   \n896      ?            C     A       0        85              T         ?   \n897      ?            C     A       0        85              T         ?   \n\n    formability  strength non-ageing  ...  s  p  shape  thick   width    len  \\\n0             ?       0.0          ?  ...  ?  ?   COIL  0.700   610.0    0.0   \n1             2       0.0          ?  ...  ?  ?   COIL  3.200   610.0    0.0   \n2             2       0.0          ?  ...  ?  ?  SHEET  0.700  1300.0  762.0   \n3             ?       0.0          ?  ...  ?  ?   COIL  2.801   385.1    0.0   \n4             ?       0.0          ?  ...  ?  ?  SHEET  0.801   255.0  269.0   \n..          ...       ...        ...  ... .. ..    ...    ...     ...    ...   \n893           3       0.0          ?  ...  ?  ?  SHEET  1.599   610.0  762.0   \n894           3       0.0          ?  ...  ?  ?  SHEET  1.601   830.0  880.0   \n895           2       0.0          ?  ...  ?  ?  SHEET  1.599   150.0  762.0   \n896           ?       0.0          ?  ...  ?  ?   COIL  0.400    20.0    0.0   \n897           ?       0.0          ?  ...  ?  ?   COIL  4.000   610.0    0.0   \n\n    oil bore packing class  \n0     ?    0       ?     3  \n1     ?    0       ?     3  \n2     ?    0       ?     3  \n3     ?    0       ?     3  \n4     ?    0       ?     3  \n..   ..  ...     ...   ...  \n893   ?    0       ?     2  \n894   ?    0       ?     2  \n895   ?    0       ?     2  \n896   ?    0       ?     U  \n897   ?  500       ?     U  \n\n[898 rows x 39 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family</th>\n      <th>product-type</th>\n      <th>steel</th>\n      <th>carbon</th>\n      <th>hardness</th>\n      <th>temper_rolling</th>\n      <th>condition</th>\n      <th>formability</th>\n      <th>strength</th>\n      <th>non-ageing</th>\n      <th>...</th>\n      <th>s</th>\n      <th>p</th>\n      <th>shape</th>\n      <th>thick</th>\n      <th>width</th>\n      <th>len</th>\n      <th>oil</th>\n      <th>bore</th>\n      <th>packing</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>?</td>\n      <td>C</td>\n      <td>A</td>\n      <td>8</td>\n      <td>0</td>\n      <td>?</td>\n      <td>S</td>\n      <td>?</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>COIL</td>\n      <td>0.700</td>\n      <td>610.0</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>?</td>\n      <td>C</td>\n      <td>R</td>\n      <td>0</td>\n      <td>0</td>\n      <td>?</td>\n      <td>S</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>COIL</td>\n      <td>3.200</td>\n      <td>610.0</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>?</td>\n      <td>C</td>\n      <td>R</td>\n      <td>0</td>\n      <td>0</td>\n      <td>?</td>\n      <td>S</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>SHEET</td>\n      <td>0.700</td>\n      <td>1300.0</td>\n      <td>762.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>?</td>\n      <td>C</td>\n      <td>A</td>\n      <td>0</td>\n      <td>60</td>\n      <td>T</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>COIL</td>\n      <td>2.801</td>\n      <td>385.1</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>?</td>\n      <td>C</td>\n      <td>A</td>\n      <td>0</td>\n      <td>60</td>\n      <td>T</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>SHEET</td>\n      <td>0.801</td>\n      <td>255.0</td>\n      <td>269.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>893</th>\n      <td>?</td>\n      <td>C</td>\n      <td>R</td>\n      <td>0</td>\n      <td>0</td>\n      <td>?</td>\n      <td>S</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>SHEET</td>\n      <td>1.599</td>\n      <td>610.0</td>\n      <td>762.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>?</td>\n      <td>C</td>\n      <td>R</td>\n      <td>0</td>\n      <td>0</td>\n      <td>?</td>\n      <td>S</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>SHEET</td>\n      <td>1.601</td>\n      <td>830.0</td>\n      <td>880.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>?</td>\n      <td>C</td>\n      <td>V</td>\n      <td>0</td>\n      <td>0</td>\n      <td>?</td>\n      <td>S</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>SHEET</td>\n      <td>1.599</td>\n      <td>150.0</td>\n      <td>762.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>?</td>\n      <td>C</td>\n      <td>A</td>\n      <td>0</td>\n      <td>85</td>\n      <td>T</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>COIL</td>\n      <td>0.400</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>0</td>\n      <td>?</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>?</td>\n      <td>C</td>\n      <td>A</td>\n      <td>0</td>\n      <td>85</td>\n      <td>T</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>COIL</td>\n      <td>4.000</td>\n      <td>610.0</td>\n      <td>0.0</td>\n      <td>?</td>\n      <td>500</td>\n      <td>?</td>\n      <td>U</td>\n    </tr>\n  </tbody>\n</table>\n<p>898 rows × 39 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas.core.frame.DataFrame\n",
    "ds_DF = ds1.get_data(dataset_format='dataframe')[0]\n",
    "\n",
    "ds_ND_arr = ds1.get_data(dataset_format='array')[0]\n",
    "\n",
    "numpy.savetxt(\"./datasets_study1/ds_1.csv\", ds_ND_arr, delimiter=\",\")\n",
    "ds_ND_arr_loaded = numpy.loadtxt(\"./datasets_study1/ds_1.csv\",delimiter=\",\")\n",
    "ds_ND_arr_loaded\n",
    "# it messes with categorical columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# pip install -U pymfe\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "y = data.target\n",
    "X = data.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "              0             1              2            3             4    \\\n0  attr_conc.mean  attr_conc.sd  attr_ent.mean  attr_ent.sd  attr_to_inst   \n1        0.209805      0.119588       2.277191     0.061039      0.026667   \n\n              5             6             7           8           9    ...  \\\n0  best_node.mean  best_node.sd  can_cor.mean  can_cor.sd  cat_to_num  ...   \n1        0.666667           0.0      0.728009    0.363187         0.0  ...   \n\n                 101              102            103       104       105  \\\n0  tree_imbalance.sd  tree_shape.mean  tree_shape.sd  var.mean    var.sd   \n1           0.133007         0.270833        0.10712  1.143239  1.332546   \n\n                   106                107       108              109  \\\n0  var_importance.mean  var_importance.sd  w_lambda  worst_node.mean   \n1                 0.25           0.286901  0.023439         0.586667   \n\n             110  \n0  worst_node.sd  \n1       0.081952  \n\n[2 rows x 111 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>101</th>\n      <th>102</th>\n      <th>103</th>\n      <th>104</th>\n      <th>105</th>\n      <th>106</th>\n      <th>107</th>\n      <th>108</th>\n      <th>109</th>\n      <th>110</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>attr_conc.mean</td>\n      <td>attr_conc.sd</td>\n      <td>attr_ent.mean</td>\n      <td>attr_ent.sd</td>\n      <td>attr_to_inst</td>\n      <td>best_node.mean</td>\n      <td>best_node.sd</td>\n      <td>can_cor.mean</td>\n      <td>can_cor.sd</td>\n      <td>cat_to_num</td>\n      <td>...</td>\n      <td>tree_imbalance.sd</td>\n      <td>tree_shape.mean</td>\n      <td>tree_shape.sd</td>\n      <td>var.mean</td>\n      <td>var.sd</td>\n      <td>var_importance.mean</td>\n      <td>var_importance.sd</td>\n      <td>w_lambda</td>\n      <td>worst_node.mean</td>\n      <td>worst_node.sd</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.209805</td>\n      <td>0.119588</td>\n      <td>2.277191</td>\n      <td>0.061039</td>\n      <td>0.026667</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.728009</td>\n      <td>0.363187</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.133007</td>\n      <td>0.270833</td>\n      <td>0.10712</td>\n      <td>1.143239</td>\n      <td>1.332546</td>\n      <td>0.25</td>\n      <td>0.286901</td>\n      <td>0.023439</td>\n      <td>0.586667</td>\n      <td>0.081952</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 111 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfe1 = MFE(random_state=42)\n",
    "mfe1.fit(X, y) # , cat_cols=[1,2,3] ?? it would be beneficial\n",
    "ft1 = mfe1.extract()\n",
    "pd.DataFrame(ft1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------------------------------+\n",
      "|    Group    | Meta-feature name |                Description                 |\n",
      "+=============+===================+============================================+\n",
      "| complexity  | c1                | Compute the entropy of class proportions.  |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | c2                | Compute the imbalance ratio.               |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | cls_coef          | Clustering coefficient.                    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | density           | Average density of the network.            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | f1                | Maximum Fisher's discriminant ratio.       |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | f1v               | Directional-vector maximum Fisher's        |\n",
      "|             |                   | discriminant ratio.                        |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | f2                | Volume of the overlapping region.          |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | f3                | Compute feature maximum individual         |\n",
      "|             |                   | efficiency.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | f4                | Compute the collective feature efficiency. |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | hubs              | Hub score.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | l1                | Sum of error distance by linear            |\n",
      "|             |                   | programming.                               |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | l2                | Compute the OVO subsets error rate of      |\n",
      "|             |                   | linear classifier.                         |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | l3                | Non-Linearity of a linear classifier.      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | lsc               | Local set average cardinality.             |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | n1                | Compute the fraction of borderline points. |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | n2                | Ratio of intra and extra class nearest     |\n",
      "|             |                   | neighbor distance.                         |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | n3                | Error rate of the nearest neighbor         |\n",
      "|             |                   | classifier.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | n4                | Compute the non-linearity of the k-NN      |\n",
      "|             |                   | Classifier.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | t1                | Fraction of hyperspheres covering data.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | t2                | Compute the average number of features per |\n",
      "|             |                   | dimension.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | t3                | Compute the average number of PCA          |\n",
      "|             |                   | dimensions per points.                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| complexity  | t4                | Compute the ratio of the PCA dimension to  |\n",
      "|             |                   | the original dimension.                    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | leaves            | Compute the number of leaf nodes in the DT |\n",
      "|             |                   | model.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | leaves_branch     | Compute the size of branches in the DT     |\n",
      "|             |                   | model.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | leaves_corrob     | Compute the leaves corroboration of the DT |\n",
      "|             |                   | model.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | leaves_homo       | Compute the DT model Homogeneity for every |\n",
      "|             |                   | leaf node.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | leaves_per_class  | Compute the proportion of leaves per class |\n",
      "|             |                   | in DT model.                               |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | nodes             | Compute the number of non-leaf nodes in DT |\n",
      "|             |                   | model.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | nodes_per_attr    | Compute the ratio of nodes per number of   |\n",
      "|             |                   | attributes in DT model.                    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | nodes_per_inst    | Compute the ratio of non-leaf nodes per    |\n",
      "|             |                   | number of instances in DT model.           |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | nodes_per_level   | Compute the ratio of number of nodes per   |\n",
      "|             |                   | tree level in DT model.                    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | nodes_repeated    | Compute the number of repeated nodes in DT |\n",
      "|             |                   | model.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | tree_depth        | Compute the depth of every node in the DT  |\n",
      "|             |                   | model.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | tree_imbalance    | Compute the tree imbalance for each leaf   |\n",
      "|             |                   | node.                                      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | tree_shape        | Compute the tree shape for every leaf      |\n",
      "|             |                   | node.                                      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| model-based | var_importance    | Compute the features importance of the DT  |\n",
      "|             |                   | model for each attribute.                  |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | attr_to_inst      | Compute the ratio between the number of    |\n",
      "|             |                   | attributes.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | cat_to_num        | Compute the ratio between the number of    |\n",
      "|             |                   | categoric and numeric features.            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | freq_class        | Compute the relative frequency of each     |\n",
      "|             |                   | distinct class.                            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | inst_to_attr      | Compute the ratio between the number of    |\n",
      "|             |                   | instances and attributes.                  |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | nr_attr           | Compute the total number of attributes.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | nr_bin            | Compute the number of binary attributes.   |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | nr_cat            | Compute the number of categorical          |\n",
      "|             |                   | attributes.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | nr_class          | Compute the number of distinct classes.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | nr_inst           | Compute the number of instances (rows) in  |\n",
      "|             |                   | the dataset.                               |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | nr_num            | Compute the number of numeric features.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| general     | num_to_cat        | Compute the number of numerical and        |\n",
      "|             |                   | categorical features.                      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| itemset     | one_itemset       | Compute the one itemset meta-feature.      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| itemset     | two_itemset       | Compute the two itemset meta-feature.      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | best_node         | Performance of a the best single decision  |\n",
      "|             |                   | tree node.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | elite_nn          | Performance of Elite Nearest Neighbor.     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | linear_discr      | Performance of the Linear Discriminant     |\n",
      "|             |                   | classifier.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | naive_bayes       | Performance of the Naive Bayes classifier. |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | one_nn            | Performance of the 1-Nearest Neighbor      |\n",
      "|             |                   | classifier.                                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | random_node       | Performance of the single decision tree    |\n",
      "|             |                   | node model induced by a random attribute.  |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| landmarking | worst_node        | Performance of the single decision tree    |\n",
      "|             |                   | node model induced by the worst            |\n",
      "|             |                   | informative attribute.                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| concept     | cohesiveness      | Compute the improved version of the        |\n",
      "|             |                   | weighted distance, that captures how dense |\n",
      "|             |                   | or sparse is the example distribution.     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| concept     | conceptvar        | Compute the concept variation that         |\n",
      "|             |                   | estimates the variability of class labels  |\n",
      "|             |                   | among examples.                            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| concept     | impconceptvar     | Compute the improved concept variation     |\n",
      "|             |                   | that estimates the variability of class    |\n",
      "|             |                   | labels among examples.                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| concept     | wg_dist           | Compute the weighted distance, that        |\n",
      "|             |                   | captures how dense or sparse is the        |\n",
      "|             |                   | example distribution.                      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | ch                | Compute the Calinski and Harabasz index.   |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | int               | Compute the INT index.                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | nre               | Compute the normalized relative entropy.   |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | pb                | Compute the pearson correlation between    |\n",
      "|             |                   | class matching and instance distances.     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | sc                | Compute the number of clusters with size   |\n",
      "|             |                   | smaller than a given size.                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | sil               | Compute the mean silhouette value.         |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | vdb               | Compute the Davies and Bouldin Index.      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| clustering  | vdu               | Compute the Dunn Index.                    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | attr_conc         | Compute concentration coef. of each pair   |\n",
      "|             |                   | of distinct attributes.                    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | attr_ent          | Compute Shannon's entropy for each         |\n",
      "|             |                   | predictive attribute.                      |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | class_conc        | Compute concentration coefficient between  |\n",
      "|             |                   | each attribute and class.                  |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | class_ent         | Compute target attribute Shannon's         |\n",
      "|             |                   | entropy.                                   |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | eq_num_attr       | Compute the number of attributes           |\n",
      "|             |                   | equivalent for a predictive task.          |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | joint_ent         | Compute the joint entropy between each     |\n",
      "|             |                   | attribute and class.                       |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | mut_inf           | Compute the mutual information between     |\n",
      "|             |                   | each attribute and target.                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| info-theory | ns_ratio          | Compute the noisiness of attributes.       |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | can_cor           | Compute canonical correlations of data.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | cor               | Compute the absolute value of the          |\n",
      "|             |                   | correlation of distinct dataset column     |\n",
      "|             |                   | pairs.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | cov               | Compute the absolute value of the          |\n",
      "|             |                   | covariance of distinct dataset attribute   |\n",
      "|             |                   | pairs.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | eigenvalues       | Compute the eigenvalues of covariance      |\n",
      "|             |                   | matrix from dataset.                       |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | g_mean            | Compute the geometric mean of each         |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | gravity           | Compute the distance between minority and  |\n",
      "|             |                   | majority classes center of mass.           |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | h_mean            | Compute the harmonic mean of each          |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | iq_range          | Compute the interquartile range (IQR) of   |\n",
      "|             |                   | each attribute.                            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | kurtosis          | Compute the kurtosis of each attribute.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | lh_trace          | Compute the Lawley-Hotelling trace.        |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | mad               | Compute the Median Absolute Deviation      |\n",
      "|             |                   | (MAD) adjusted by a factor.                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | max               | Compute the maximum value from each        |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | mean              | Compute the mean value of each attribute.  |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | median            | Compute the median value from each         |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | min               | Compute the minimum value from each        |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | nr_cor_attr       | Compute the number of distinct highly      |\n",
      "|             |                   | correlated pair of attributes.             |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | nr_disc           | Compute the number of canonical            |\n",
      "|             |                   | correlation between each attribute and     |\n",
      "|             |                   | class.                                     |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | nr_norm           | Compute the number of attributes normally  |\n",
      "|             |                   | distributed based in a given method.       |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | nr_outliers       | Compute the number of attributes with at   |\n",
      "|             |                   | least one outlier value.                   |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | p_trace           | Compute the Pillai's trace.                |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | range             | Compute the range (max - min) of each      |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | roy_root          | Compute the Roy's largest root.            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | sd                | Compute the standard deviation of each     |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | sd_ratio          | Compute a statistical test for homogeneity |\n",
      "|             |                   | of covariances.                            |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | skewness          | Compute the skewness for each attribute.   |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | sparsity          | Compute (possibly normalized) sparsity     |\n",
      "|             |                   | metric for each attribute.                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | t_mean            | Compute the trimmed mean of each           |\n",
      "|             |                   | attribute.                                 |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | var               | Compute the variance of each attribute.    |\n",
      "+-------------+-------------------+--------------------------------------------+\n",
      "| statistical | w_lambda          | Compute the Wilks' Lambda value.           |\n",
      "+-------------+-------------------+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mfe1.metafeature_description()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "('landmarking',\n 'general',\n 'statistical',\n 'model-based',\n 'info-theory',\n 'relative',\n 'clustering',\n 'complexity',\n 'itemset',\n 'concept')"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfe1.valid_groups()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "('mean',\n 'nanmean',\n 'sd',\n 'nansd',\n 'var',\n 'nanvar',\n 'count',\n 'nancount',\n 'histogram',\n 'nanhistogram',\n 'iq_range',\n 'naniq_range',\n 'kurtosis',\n 'nankurtosis',\n 'max',\n 'nanmax',\n 'median',\n 'nanmedian',\n 'min',\n 'nanmin',\n 'quantiles',\n 'nanquantiles',\n 'range',\n 'nanrange',\n 'skewness',\n 'nanskewness',\n 'sum',\n 'nansum',\n 'powersum',\n 'pnorm',\n 'nanpowersum',\n 'nanpnorm')"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfe1.valid_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "DS_path = r'C:\\Users\\Trogwald\\Desktop\\push_it\\AutoML\\experiments\\DS_CD\\2 classes\\credit-g.csv'\n",
    "DS = pd.read_csv(DS_path, skiprows=0).values\n",
    "x=DS\n",
    "y=DS[:, 20]\n",
    "num_features=[1, 4, 7, 10, 12, 15, 17]\n",
    "cat_features=[0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19]\n",
    "\n",
    "mfe2 = MFE(groups='all')\n",
    "mfe2.fit(x,  suppress_warnings=True)\n",
    "mfe2.extract(cat_cols=cat_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_summary.py:261: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurt_val = scipy.stats.kurtosis(values, bias=bias)\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'best_node' with summary 'kurtosis'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'best_node' with summary 'nankurtosis'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_summary.py:198: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skew_val = scipy.stats.skew(values, bias=bias)\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'best_node' with summary 'skewness'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'best_node' with summary 'nanskewness'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_summary.py:261: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  kurt_val = scipy.stats.kurtosis(values, bias=bias)\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'freq_class' with summary 'kurtosis'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'freq_class' with summary 'nankurtosis'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_summary.py:198: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  skew_val = scipy.stats.skew(values, bias=bias)\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'freq_class' with summary 'skewness'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Trogwald\\anaconda3\\envs\\openml_db\\lib\\site-packages\\pymfe\\_internal.py:731: RuntimeWarning: Can't summarize feature 'freq_class' with summary 'nanskewness'. Will set it as 'np.nan'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "              0                      1                      2     \\\n0  attr_conc.count  attr_conc.histogram.0  attr_conc.histogram.1   \n1               12                   0.25                   0.25   \n\n                    3                      4                      5     \\\n0  attr_conc.histogram.2  attr_conc.histogram.3  attr_conc.histogram.4   \n1                    0.0                    0.0                   0.25   \n\n                    6                      7                      8     \\\n0  attr_conc.histogram.5  attr_conc.histogram.6  attr_conc.histogram.7   \n1               0.083333                    0.0                    0.0   \n\n                    9     ...              3978                       3979  \\\n0  attr_conc.histogram.8  ...  worst_node.range  worst_node.range.relative   \n1                    0.0  ...               0.2                        5.5   \n\n            3980                    3981                 3982  \\\n0  worst_node.sd  worst_node.sd.relative  worst_node.skewness   \n1       0.084327                     6.0             0.094868   \n\n                           3983            3984                     3985  \\\n0  worst_node.skewness.relative  worst_node.sum  worst_node.sum.relative   \n1                           5.0        5.733333                      2.0   \n\n             3986                     3987  \n0  worst_node.var  worst_node.var.relative  \n1        0.007111                      6.0  \n\n[2 rows x 3988 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>3978</th>\n      <th>3979</th>\n      <th>3980</th>\n      <th>3981</th>\n      <th>3982</th>\n      <th>3983</th>\n      <th>3984</th>\n      <th>3985</th>\n      <th>3986</th>\n      <th>3987</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>attr_conc.count</td>\n      <td>attr_conc.histogram.0</td>\n      <td>attr_conc.histogram.1</td>\n      <td>attr_conc.histogram.2</td>\n      <td>attr_conc.histogram.3</td>\n      <td>attr_conc.histogram.4</td>\n      <td>attr_conc.histogram.5</td>\n      <td>attr_conc.histogram.6</td>\n      <td>attr_conc.histogram.7</td>\n      <td>attr_conc.histogram.8</td>\n      <td>...</td>\n      <td>worst_node.range</td>\n      <td>worst_node.range.relative</td>\n      <td>worst_node.sd</td>\n      <td>worst_node.sd.relative</td>\n      <td>worst_node.skewness</td>\n      <td>worst_node.skewness.relative</td>\n      <td>worst_node.sum</td>\n      <td>worst_node.sum.relative</td>\n      <td>worst_node.var</td>\n      <td>worst_node.var.relative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.25</td>\n      <td>0.083333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.2</td>\n      <td>5.5</td>\n      <td>0.084327</td>\n      <td>6.0</td>\n      <td>0.094868</td>\n      <td>5.0</td>\n      <td>5.733333</td>\n      <td>2.0</td>\n      <td>0.007111</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 3988 columns</p>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfe3 = MFE(groups=\"all\", summary=\"all\", random_state=42)\n",
    "mfe3.fit(X, y)\n",
    "ft3 = mfe3.extract()\n",
    "pd.DataFrame(ft3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(arff.load(open(ds1.data_file, 'r')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "[('family', ['?', 'GB', 'GK', 'GS', 'TN', 'ZA', 'ZF', 'ZH', 'ZM', 'ZS']),\n ('product-type', ['C', 'H', 'G']),\n ('steel', ['?', 'R', 'A', 'U', 'K', 'M', 'S', 'W', 'V']),\n ('carbon', 'REAL'),\n ('hardness', 'REAL'),\n ('temper_rolling', ['?', 'T']),\n ('condition', ['?', 'S', 'A', 'X']),\n ('formability', ['?', '1', '2', '3', '4', '5']),\n ('strength', 'REAL'),\n ('non-ageing', ['?', 'N']),\n ('surface-finish', ['?', 'P', 'M']),\n ('surface-quality', ['?', 'D', 'E', 'F', 'G']),\n ('enamelability', ['?', '1', '2', '3', '4', '5']),\n ('bc', ['?', 'Y']),\n ('bf', ['?', 'Y']),\n ('bt', ['?', 'Y']),\n ('bw%2Fme', ['?', 'B', 'M']),\n ('bl', ['?', 'Y']),\n ('m', ['?', 'Y']),\n ('chrom', ['?', 'C']),\n ('phos', ['?', 'P']),\n ('cbond', ['?', 'Y']),\n ('marvi', ['?', 'Y']),\n ('exptl', ['?', 'Y']),\n ('ferro', ['?', 'Y']),\n ('corr', ['?', 'Y']),\n ('blue%2Fbright%2Fvarn%2Fclean', ['?', 'B', 'R', 'V', 'C']),\n ('lustre', ['?', 'Y']),\n ('jurofm', ['?', 'Y']),\n ('s', ['?', 'Y']),\n ('p', ['?', 'Y']),\n ('shape', ['COIL', 'SHEET']),\n ('thick', 'REAL'),\n ('width', 'REAL'),\n ('len', 'REAL'),\n ('oil', ['?', 'Y', 'N']),\n ('bore', ['0', '500', '600', '760']),\n ('packing', ['?', '1', '2', '3']),\n ('class', ['1', '2', '3', '4', '5', 'U'])]"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ['description', 'relation', 'attributes', 'data']\n",
    "loaded_dataset = arff.load(open(ds1.data_file, 'r'))\n",
    "loaded_dataset['attributes']\n",
    "# the only useful thing from arff file is 'data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}